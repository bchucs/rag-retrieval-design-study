[
  {
    "question": "What is the main advantage of the Share approach over traditional LoRA methods?",
    "answer": "The Share approach achieves up to 100x parameter reduction and 281x memory savings compared to traditional LoRA methods while maintaining performance comparable to jointly trained models. It enables seamless adaptation across multiple tasks and modalities by dynamically updating a single shared low-rank subspace.",
    "doc_id": "2602.06043v1",
    "gold_doc_ids": [
      "2602.06043v1"
    ],
    "title": "Shared LoRA Subspaces for almost Strict Continual Learning",
    "question_id": "q_000"
  },
  {
    "question": "In which domains were the experiments validating the effectiveness of Share conducted?",
    "answer": "Experiments validating the effectiveness of Share were conducted across image classification, natural language understanding, 3D pose estimation, and text-to-image generation. This demonstrates its applicability in diverse areas of machine learning.",
    "doc_id": "2602.06043v1",
    "gold_doc_ids": [
      "2602.06043v1"
    ],
    "title": "Shared LoRA Subspaces for almost Strict Continual Learning",
    "question_id": "q_001"
  },
  {
    "question": "What type of communication graph does DyTopo reconstruct, and how is it utilized in the multi-agent framework?",
    "answer": "DyTopo reconstructs a sparse directed communication graph at each round, which is utilized to route private messages based on the semantic matching of lightweight natural-language query and key descriptors outputted by each agent. This approach allows for communication patterns that are better matched to the stage-dependent needs of iterative problem solving.",
    "doc_id": "2602.06039v1",
    "gold_doc_ids": [
      "2602.06039v1"
    ],
    "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
    "question_id": "q_002"
  },
  {
    "question": "What performance improvement does DyTopo achieve over the strongest baseline in the evaluated benchmarks?",
    "answer": "DyTopo consistently outperforms the strongest baseline by an average of 6.2 across code generation and mathematical reasoning benchmarks, demonstrating its effectiveness in multi-agent reasoning tasks.",
    "doc_id": "2602.06039v1",
    "gold_doc_ids": [
      "2602.06039v1"
    ],
    "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
    "question_id": "q_003"
  },
  {
    "question": "What is the novel extension of canonical Embodied Question Answering (EQA) proposed in the paper?",
    "answer": "The novel extension proposed is the multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which focuses on effective communication among multiple heterogeneous robots to coordinate their efforts without redundancy.",
    "doc_id": "2602.06038v1",
    "gold_doc_ids": [
      "2602.06038v1"
    ],
    "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction",
    "question_id": "q_004"
  },
  {
    "question": "What framework does the paper introduce to address the MM-EQA problem?",
    "answer": "The paper introduces CommCP, a novel LLM-based decentralized communication framework designed specifically for the MM-EQA problem, which employs conformal prediction to enhance communication reliability.",
    "doc_id": "2602.06038v1",
    "gold_doc_ids": [
      "2602.06038v1"
    ],
    "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction",
    "question_id": "q_005"
  },
  {
    "question": "What are the three budget tiers offered by BudgetMem for memory modules?",
    "answer": "BudgetMem offers three budget tiers for memory modules: Low, Mid, and High. These tiers allow for explicit control over performance-cost trade-offs in runtime memory utilization.",
    "doc_id": "2602.06025v1",
    "gold_doc_ids": [
      "2602.06025v1"
    ],
    "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
    "question_id": "q_006"
  },
  {
    "question": "How does BudgetMem achieve performance-cost control in memory processing?",
    "answer": "BudgetMem structures memory processing as a set of memory modules and uses a lightweight router for budget-tier routing across these modules. This routing balances task performance and memory construction cost, implemented as a compact neural policy trained with reinforcement learning.",
    "doc_id": "2602.06025v1",
    "gold_doc_ids": [
      "2602.06025v1"
    ],
    "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
    "question_id": "q_007"
  },
  {
    "question": "What is the purpose of the data-driven discrete-event simulator (DES) developed in this study?",
    "answer": "The DES models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. It is used to examine the impact of a robot-based shooter intervention strategy and enables scalable evaluation and learning of intervention strategies.",
    "doc_id": "2602.06023v1",
    "gold_doc_ids": [
      "2602.06023v1"
    ],
    "title": "Learning Event-Based Shooter Models from Virtual Reality Experiments",
    "question_id": "q_008"
  },
  {
    "question": "What limitations does the research address regarding evaluating new interventions in virtual reality?",
    "answer": "The research addresses the limitation of needing to recruit new participant cohorts for each condition, which makes large-scale or iterative evaluation difficult. This is especially challenging when learning effective intervention strategies that typically require many training episodes.",
    "doc_id": "2602.06023v1",
    "gold_doc_ids": [
      "2602.06023v1"
    ],
    "title": "Learning Event-Based Shooter Models from Virtual Reality Experiments",
    "question_id": "q_009"
  },
  {
    "question": "What is the primary purpose of the CORAL method introduced in the research paper?",
    "answer": "The primary purpose of the CORAL method is to provide a regularized inference-time steering technique that captures distributed correctness signals from model internal activations to improve accuracy and calibration of large language models. It aims to enhance performance in multiple-choice question answering (MCQA) tasks without the need for retraining.",
    "doc_id": "2602.06022v1",
    "gold_doc_ids": [
      "2602.06022v1"
    ],
    "title": "Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering",
    "question_id": "q_010"
  },
  {
    "question": "What improvements in accuracy and expected calibration error (ECE) does CORAL achieve on average?",
    "answer": "CORAL consistently improves accuracy by 10% and reduces expected calibration error (ECE) by 50% on average across three 7B-parameter models. Additionally, it shows an average of 14% accuracy improvements and 49% ECE improvements when applied to four held-out benchmark test sets.",
    "doc_id": "2602.06022v1",
    "gold_doc_ids": [
      "2602.06022v1"
    ],
    "title": "Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering",
    "question_id": "q_011"
  },
  {
    "question": "What is identified as a key mechanism for restoring stability in Thompson sampling?",
    "answer": "Optimism is identified as a key mechanism for restoring stability in Thompson sampling, which is essential for valid asymptotic inference in multi-armed bandits.",
    "doc_id": "2602.06014v1",
    "gold_doc_ids": [
      "2602.06014v1"
    ],
    "title": "Optimism Stabilizes Thompson Sampling for Adaptive Inference",
    "question_id": "q_012"
  },
  {
    "question": "What does the paper conclude about the implementation of optimism in Thompson sampling?",
    "answer": "The paper concludes that suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while only incurring a mild additional regret cost.",
    "doc_id": "2602.06014v1",
    "gold_doc_ids": [
      "2602.06014v1"
    ],
    "title": "Optimism Stabilizes Thompson Sampling for Adaptive Inference",
    "question_id": "q_013"
  },
  {
    "question": "What limitations of the absolute pointwise scoring standard are identified in the research?",
    "answer": "The absolute pointwise scoring standard is limited due to stochastic inconsistency and poor alignment with human perception. These limitations necessitate the adoption of a new evaluation approach.",
    "doc_id": "2602.06013v1",
    "gold_doc_ids": [
      "2602.06013v1"
    ],
    "title": "GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?",
    "question_id": "q_014"
  },
  {
    "question": "What performance improvement does GenArena achieve compared to pointwise methods?",
    "answer": "GenArena boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the LMArena leaderboard, which drastically surpasses the 0.36 correlation of pointwise methods.",
    "doc_id": "2602.06013v1",
    "gold_doc_ids": [
      "2602.06013v1"
    ],
    "title": "GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?",
    "question_id": "q_015"
  },
  {
    "question": "What is the primary purpose of the AgenticPay framework introduced in the research paper?",
    "answer": "The primary purpose of the AgenticPay framework is to serve as a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. It aims to evaluate language-mediated economic interactions among multiple agents in a principled setting.",
    "doc_id": "2602.06008v1",
    "gold_doc_ids": [
      "2602.06008v1"
    ],
    "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions",
    "question_id": "q_016"
  },
  {
    "question": "What types of tasks does the AgenticPay framework support?",
    "answer": "The AgenticPay framework supports a diverse suite of over 110 tasks, which range from bilateral bargaining to many-to-many markets. It also includes structured action extraction and metrics for feasibility, efficiency, and welfare.",
    "doc_id": "2602.06008v1",
    "gold_doc_ids": [
      "2602.06008v1"
    ],
    "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions",
    "question_id": "q_017"
  },
  {
    "question": "What two datasets were used for experimentation in the study on Speech Emotion Recognition?",
    "answer": "The two datasets used for experimentation are IEMOCAP for English and ShEMO for Persian. These datasets were employed to evaluate the effectiveness of Whisper representations in speech emotion recognition.",
    "doc_id": "2602.06000v1",
    "gold_doc_ids": [
      "2602.06000v1"
    ],
    "title": "Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods",
    "question_id": "q_018"
  },
  {
    "question": "What improvements did the multi-head QKV architecture achieve on the ShEMO dataset?",
    "answer": "The multi-head QKV architecture achieved state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. This showcases the effectiveness of the proposed approach in enhancing emotion recognition performance.",
    "doc_id": "2602.06000v1",
    "gold_doc_ids": [
      "2602.06000v1"
    ],
    "title": "Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods",
    "question_id": "q_019"
  },
  {
    "question": "What is the main contribution of the proposed model 'Diamond Maps' in the context of reward alignment?",
    "answer": "'Diamond Maps' are stochastic flow map models designed to enable efficient and accurate alignment to arbitrary rewards at inference time, addressing the challenge of post-training adaptation to user preferences or constraints.",
    "doc_id": "2602.05993v1",
    "gold_doc_ids": [
      "2602.05993v1"
    ],
    "title": "Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps",
    "question_id": "q_020"
  },
  {
    "question": "How do Diamond Maps compare to existing methods in terms of efficiency and performance?",
    "answer": "Diamond Maps achieve stronger reward alignment performance and scale better than existing methods, making them a practical solution for adapting generative models to arbitrary preferences and constraints at inference time.",
    "doc_id": "2602.05993v1",
    "gold_doc_ids": [
      "2602.05993v1"
    ],
    "title": "Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps",
    "question_id": "q_021"
  },
  {
    "question": "What is the primary purpose of the RISE-Video benchmark?",
    "answer": "The primary purpose of the RISE-Video benchmark is to evaluate generative video models' capacity to internalize and reason over implicit world rules, shifting the focus from aesthetic quality to deep cognitive reasoning.",
    "doc_id": "2602.05986v1",
    "gold_doc_ids": [
      "2602.05986v1"
    ],
    "title": "RISE-Video: Can Video Generators Decode Implicit World Rules?",
    "question_id": "q_022"
  },
  {
    "question": "How many human-annotated samples does RISE-Video comprise and how many categories does it span?",
    "answer": "RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories.",
    "doc_id": "2602.05986v1",
    "gold_doc_ids": [
      "2602.05986v1"
    ],
    "title": "RISE-Video: Can Video Generators Decode Implicit World Rules?",
    "question_id": "q_023"
  },
  {
    "question": "What is the name of the model introduced in the paper for improving motorway traffic forecasting?",
    "answer": "The model introduced in the paper is called the Geographically-aware Transformer-based Traffic Forecasting (GATTF) model.",
    "doc_id": "2602.05983v1",
    "gold_doc_ids": [
      "2602.05983v1"
    ],
    "title": "Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins",
    "question_id": "q_024"
  },
  {
    "question": "What type of data was used to evaluate the GATTF model?",
    "answer": "The GATTF model was evaluated using real-time data from the Geneva motorway network in Switzerland.",
    "doc_id": "2602.05983v1",
    "gold_doc_ids": [
      "2602.05983v1"
    ],
    "title": "Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins",
    "question_id": "q_025"
  },
  {
    "question": "What is the main application area of the Clifford Kolmogorov-Arnold Network (ClKAN)?",
    "answer": "The ClKAN finds application in scientific discovery and engineering. It serves as a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces.",
    "doc_id": "2602.05977v1",
    "gold_doc_ids": [
      "2602.05977v1"
    ],
    "title": "Clifford Kolmogorov-Arnold Networks",
    "question_id": "q_026"
  },
  {
    "question": "What method does ClKAN propose to address exponential scaling in higher dimensional algebras?",
    "answer": "ClKAN proposes the use of Randomized Quasi Monte Carlo grid generation to tackle the exponential scaling associated with higher dimensional algebras. This method helps in efficiently managing the computational challenges posed by such spaces.",
    "doc_id": "2602.05977v1",
    "gold_doc_ids": [
      "2602.05977v1"
    ],
    "title": "Clifford Kolmogorov-Arnold Networks",
    "question_id": "q_027"
  },
  {
    "question": "How does loss scale in relation to depth in large language models (LLMs) according to the findings presented in the abstract?",
    "answer": "Loss scales inversely proportional to depth in LLMs. This relationship is likely due to functionally similar layers reducing error through ensemble averaging.",
    "doc_id": "2602.05970v1",
    "gold_doc_ids": [
      "2602.05970v1"
    ],
    "title": "Inverse Depth Scaling From Most Layers Being Similar",
    "question_id": "q_028"
  },
  {
    "question": "What architectural characteristics are suggested to potentially improve the efficiency of LLMs based on the findings of the research?",
    "answer": "The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth. This is in response to the current regime being inefficient yet robust.",
    "doc_id": "2602.05970v1",
    "gold_doc_ids": [
      "2602.05970v1"
    ],
    "title": "Inverse Depth Scaling From Most Layers Being Similar",
    "question_id": "q_029"
  },
  {
    "question": "What framework is proposed in the paper for enhancing temporal consistency in video generation?",
    "answer": "The paper proposes Localized Semantic Alignment (LSA) as a framework for fine-tuning pre-trained video generation models to enhance temporal consistency by aligning semantic features between ground-truth and generated video clips.",
    "doc_id": "2602.05966v1",
    "gold_doc_ids": [
      "2602.05966v1"
    ],
    "title": "LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation",
    "question_id": "q_030"
  },
  {
    "question": "What datasets were used to test the effectiveness of the proposed approach?",
    "answer": "The effectiveness of the proposed approach was tested on the nuScenes and KITTI datasets, demonstrating enhancements in temporal consistency in video generation.",
    "doc_id": "2602.05966v1",
    "gold_doc_ids": [
      "2602.05966v1"
    ],
    "title": "LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation",
    "question_id": "q_031"
  },
  {
    "question": "What is the purpose of the Learning to Share (LTS) mechanism proposed in the paper?",
    "answer": "The Learning to Share (LTS) mechanism is designed to enable selective cross-team information reuse in parallel agentic systems while controlling context growth. It aims to improve the efficiency of these systems by reducing overlapping computations among different agent teams.",
    "doc_id": "2602.05965v1",
    "gold_doc_ids": [
      "2602.05965v1"
    ],
    "title": "Learning to Share: Selective Memory for Efficient Parallel Agentic Systems",
    "question_id": "q_032"
  },
  {
    "question": "How does the controller in the LTS mechanism determine whether to add intermediate agent steps to memory?",
    "answer": "The controller in the LTS mechanism is trained using stepwise reinforcement learning with usage-aware credit assignment. This allows it to identify which intermediate steps are globally useful across parallel executions, deciding whether to add them to the shared memory bank.",
    "doc_id": "2602.05965v1",
    "gold_doc_ids": [
      "2602.05965v1"
    ],
    "title": "Learning to Share: Selective Memory for Efficient Parallel Agentic Systems",
    "question_id": "q_033"
  },
  {
    "question": "What is the main objective of the proposed method in this research paper?",
    "answer": "The main objective is to learn a condition-dependent source distribution under the flow matching objective to better exploit rich conditioning signals in text-to-image generation. This approach aims to optimize the source distribution itself rather than relying on a standard Gaussian distribution.",
    "doc_id": "2602.05951v1",
    "gold_doc_ids": [
      "2602.05951v1"
    ],
    "title": "Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching",
    "question_id": "q_034"
  },
  {
    "question": "What are two key factors identified as critical for stable and effective learning in the proposed method?",
    "answer": "The two key factors identified are appropriate variance regularization and directional alignment between the source and target distributions. These factors help address failure modes like distributional collapse and instability when incorporating conditioning into the source.",
    "doc_id": "2602.05951v1",
    "gold_doc_ids": [
      "2602.05951v1"
    ],
    "title": "Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching",
    "question_id": "q_035"
  },
  {
    "question": "What percentage of the AI-generated hallucinations analyzed in the study were classified as Total Fabrication?",
    "answer": "66% of the AI-generated hallucinations were classified as Total Fabrication according to the five-category taxonomy developed in the study.",
    "doc_id": "2602.05930v1",
    "gold_doc_ids": [
      "2602.05930v1"
    ],
    "title": "Compound Deception in Elite Peer Review: A Failure Mode Taxonomy of 100 Fabricated Citations at NeurIPS 2025",
    "question_id": "q_036"
  },
  {
    "question": "What percentage of contaminated papers contained 1-2 hallucinations, and what does this indicate about AI usage?",
    "answer": "92% of contaminated papers contained 1-2 hallucinations, indicating minimal AI use among those papers.",
    "doc_id": "2602.05930v1",
    "gold_doc_ids": [
      "2602.05930v1"
    ],
    "title": "Compound Deception in Elite Peer Review: A Failure Mode Taxonomy of 100 Fabricated Citations at NeurIPS 2025",
    "question_id": "q_037"
  },
  {
    "question": "What methods are compared in the study to address the Capacitated Vehicle Routing Problem (CVRP)?",
    "answer": "The study compares classical and quantum Reinforcement Learning (RL) approaches, specifically implementing an Advantage Actor-Critic (A2C) agent in classical, full quantum, and hybrid variants.",
    "doc_id": "2602.05920v1",
    "gold_doc_ids": [
      "2602.05920v1"
    ],
    "title": "Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem",
    "question_id": "q_038"
  },
  {
    "question": "What metrics are used to assess the performance of the routing policies in the experiments?",
    "answer": "Performance is assessed using routing distance, route compactness, and route overlap, focusing on multi-vehicle scenarios with capacity constraints.",
    "doc_id": "2602.05920v1",
    "gold_doc_ids": [
      "2602.05920v1"
    ],
    "title": "Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem",
    "question_id": "q_039"
  },
  {
    "question": "What is the main advantage of the DFlash framework compared to existing speculative decoding methods?",
    "answer": "The main advantage of the DFlash framework is that it employs a lightweight block diffusion model for parallel drafting, which allows for efficient drafting with high-quality outputs and higher acceptance rates, overcoming the limitations of sequential autoregressive drafting.",
    "doc_id": "2602.06036v1",
    "gold_doc_ids": [
      "2602.06036v1"
    ],
    "title": "DFlash: Block Diffusion for Flash Speculative Decoding",
    "question_id": "q_040"
  },
  {
    "question": "How much acceleration does DFlash achieve compared to the state-of-the-art speculative decoding method EAGLE-3?",
    "answer": "DFlash achieves over 6x lossless acceleration and delivers up to 2.5x higher speedup compared to the state-of-the-art speculative decoding method EAGLE-3 across a range of models and tasks.",
    "doc_id": "2602.06036v1",
    "gold_doc_ids": [
      "2602.06036v1"
    ],
    "title": "DFlash: Block Diffusion for Flash Speculative Decoding",
    "question_id": "q_041"
  },
  {
    "question": "What is the primary method proposed in the paper for accelerating language model inference?",
    "answer": "The primary method proposed is converting a pretrained autoregressive language model into a fast standalone multi-token prediction model using a simple online distillation objective. This approach eliminates the need for auxiliary speculator models and complex inference pipelines.",
    "doc_id": "2602.06019v1",
    "gold_doc_ids": [
      "2602.06019v1"
    ],
    "title": "Multi-Token Prediction via Self-Distillation",
    "question_id": "q_042"
  },
  {
    "question": "How much faster do the models produced by the proposed method decode on average, and what is the associated drop in accuracy?",
    "answer": "The models produced can decode more than 3 times faster on average with less than a 5% drop in accuracy compared to single token decoding performance. This demonstrates the effectiveness of the proposed approach.",
    "doc_id": "2602.06019v1",
    "gold_doc_ids": [
      "2602.06019v1"
    ],
    "title": "Multi-Token Prediction via Self-Distillation",
    "question_id": "q_043"
  },
  {
    "question": "What factors were systematically varied in the study to evaluate the performance of large language models for PTSD severity estimation?",
    "answer": "The study systematically varied contextual knowledge, including subscale definitions and interview questions, as well as modeling strategies such as zero-shot vs few-shot, reasoning effort, model sizes, and ensemble methods.",
    "doc_id": "2602.06015v1",
    "gold_doc_ids": [
      "2602.06015v1"
    ],
    "title": "A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies",
    "question_id": "q_044"
  },
  {
    "question": "How does the reasoning effort affect the accuracy of large language models in PTSD severity estimation according to the findings?",
    "answer": "Increased reasoning effort leads to better estimation accuracy for large language models when assessing PTSD severity, as indicated by the study's findings.",
    "doc_id": "2602.06015v1",
    "gold_doc_ids": [
      "2602.06015v1"
    ],
    "title": "A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies",
    "question_id": "q_045"
  },
  {
    "question": "What is the main limitation of the widely-used fixed, predefined block schedule in inference for dLLMs?",
    "answer": "The main limitation of the fixed, predefined block schedule is that it is agnostic to semantic difficulty, leading to suboptimal performance by forcing premature commitments to uncertain positions and delaying easier positions near block boundaries.",
    "doc_id": "2602.05992v1",
    "gold_doc_ids": [
      "2602.05992v1"
    ],
    "title": "DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs",
    "question_id": "q_046"
  },
  {
    "question": "What are the two main components proposed in the paper to improve block scheduling for dLLMs?",
    "answer": "The two main components proposed are Dynamic Sliding Block (DSB), which utilizes a sliding block with a dynamic size, and DSB Cache, a training-free KV-cache mechanism tailored to DSB, both aimed at improving generation quality and inference efficiency.",
    "doc_id": "2602.05992v1",
    "gold_doc_ids": [
      "2602.05992v1"
    ],
    "title": "DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs",
    "question_id": "q_047"
  },
  {
    "question": "What is the size of the benchmark introduced in the paper, and how many queries does it comprise?",
    "answer": "The benchmark introduced in the paper is called SAGE, and it comprises 1,200 queries across four scientific domains, with a retrieval corpus of 200,000 papers.",
    "doc_id": "2602.05975v1",
    "gold_doc_ids": [
      "2602.05975v1"
    ],
    "title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents",
    "question_id": "q_048"
  },
  {
    "question": "Which retrieval methods were compared in the study, and which one outperformed the others?",
    "answer": "The study compared BM25 and LLM-based retrievers, specifically ReasonIR and gte-Qwen2-7B-instruct. BM25 significantly outperformed the LLM-based retrievers by approximately 30%.",
    "doc_id": "2602.05975v1",
    "gold_doc_ids": [
      "2602.05975v1"
    ],
    "title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents",
    "question_id": "q_049"
  },
  {
    "question": "What metrics are extracted to capture semantic navigation in the proposed framework?",
    "answer": "The framework extracts geometric and dynamical metrics including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation.",
    "doc_id": "2602.05971v1",
    "gold_doc_ids": [
      "2602.05971v1"
    ],
    "title": "Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space",
    "question_id": "q_050"
  },
  {
    "question": "What types of tasks and languages were evaluated using the proposed framework?",
    "answer": "The framework was evaluated on four datasets across different languages, spanning tasks such as Neurodegenerative, Swear verbal fluency, and Property listing tasks in Italian and German. This diversity allows for a comprehensive analysis of semantic navigation across various contexts.",
    "doc_id": "2602.05971v1",
    "gold_doc_ids": [
      "2602.05971v1"
    ],
    "title": "Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space",
    "question_id": "q_051"
  },
  {
    "question": "What is the proposed method in the paper for enhancing multilingual reasoning?",
    "answer": "The proposed method is called TRIT (Translation-Reasoning Integrated Training), which integrates the training of translation into multilingual reasoning. This framework aims to improve both multilingual question understanding and response generation without requiring external feedback or additional multilingual data.",
    "doc_id": "2602.05940v1",
    "gold_doc_ids": [
      "2602.05940v1"
    ],
    "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
    "question_id": "q_052"
  },
  {
    "question": "By how many percentage points does TRIT outperform multiple baselines on the MMATH dataset?",
    "answer": "TRIT outperforms multiple baselines by an average of 7 percentage points on the MMATH dataset, enhancing both answer correctness and language consistency.",
    "doc_id": "2602.05940v1",
    "gold_doc_ids": [
      "2602.05940v1"
    ],
    "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
    "question_id": "q_053"
  },
  {
    "question": "What is the primary focus of the research paper regarding multilingual LLMs and their responses to MCQs?",
    "answer": "The primary focus of the research paper is to investigate language-induced variation in value-laden multiple-choice question (MCQ) responses from multilingual large language models (LLMs), examining whether they behave like polyglots or express different values based on the language of the question.",
    "doc_id": "2602.05932v1",
    "gold_doc_ids": [
      "2602.05932v1"
    ],
    "title": "Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions",
    "question_id": "q_054"
  },
  {
    "question": "What new corpus is introduced in the study, and how does it differ from previous work?",
    "answer": "The study introduces the Multilingual European Value Survey (MEVS) corpus, which differs from previous work by comprising solely human-translated survey questions aligned in eight European languages, rather than relying on machine translation or ad hoc prompts.",
    "doc_id": "2602.05932v1",
    "gold_doc_ids": [
      "2602.05932v1"
    ],
    "title": "Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions",
    "question_id": "q_055"
  },
  {
    "question": "What method does KV-CoRE use to quantify the low-rank compressibility of kv-caches?",
    "answer": "KV-CoRE uses an SVD-based method to compute the optimal low-rank approximation under the Frobenius norm for quantifying the data-dependent low-rank compressibility of kv-caches.",
    "doc_id": "2602.05929v1",
    "gold_doc_ids": [
      "2602.05929v1"
    ],
    "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs",
    "question_id": "q_056"
  },
  {
    "question": "What metric is employed in the analysis to measure compressibility and its correlation with performance degradation?",
    "answer": "The Normalized Effective Rank is employed as a metric of compressibility, and it is shown to correlate strongly with performance degradation under compression.",
    "doc_id": "2602.05929v1",
    "gold_doc_ids": [
      "2602.05929v1"
    ],
    "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs",
    "question_id": "q_057"
  },
  {
    "question": "What are Codified Finite-State Machines (CFSMs) designed to do?",
    "answer": "Codified Finite-State Machines (CFSMs) are designed to automatically codify textual character profiles into finite-state machines using LLM-based coding. They extract key states and transitions directly from the profile to produce interpretable structures that enforce character consistency.",
    "doc_id": "2602.05905v1",
    "gold_doc_ids": [
      "2602.05905v1"
    ],
    "title": "Codified Finite-state Machines for Role-playing",
    "question_id": "q_058"
  },
  {
    "question": "How do Codified Probabilistic Finite-State Machines (CPFSMs) differ from CFSMs?",
    "answer": "Codified Probabilistic Finite-State Machines (CPFSMs) extend CFSMs by modeling transitions as probability distributions over states. This allows CPFSMs to better capture uncertainty and variability in character interactions.",
    "doc_id": "2602.05905v1",
    "gold_doc_ids": [
      "2602.05905v1"
    ],
    "title": "Codified Finite-state Machines for Role-playing",
    "question_id": "q_059"
  },
  {
    "question": "What is the main issue addressed by the proposed Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL) method?",
    "answer": "The main issue addressed by FaithRL is the faithfulness hallucinations that small reasoning models (SRMs) experience, particularly in intermediate reasoning steps. Existing methods inadvertently reinforce unfaithful reasoning, even when the final answer is correct.",
    "doc_id": "2602.05897v1",
    "gold_doc_ids": [
      "2602.05897v1"
    ],
    "title": "Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models",
    "question_id": "q_060"
  },
  {
    "question": "What are the key components introduced in the FaithRL method to improve reasoning in small reasoning models?",
    "answer": "FaithRL introduces step-level supervision through explicit faithfulness rewards from a process reward model and an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. These components aim to reduce hallucinations and improve the reliability of reasoning.",
    "doc_id": "2602.05897v1",
    "gold_doc_ids": [
      "2602.05897v1"
    ],
    "title": "Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models",
    "question_id": "q_061"
  },
  {
    "question": "What is the main contribution of the DFPO framework in reinforcement learning?",
    "answer": "The DFPO framework introduces a robust distributional RL method that models values as continuous flows across time steps instead of isolated quantile predictions, allowing for richer state information and more accurate advantage estimation.",
    "doc_id": "2602.05890v1",
    "gold_doc_ids": [
      "2602.05890v1"
    ],
    "title": "DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training",
    "question_id": "q_062"
  },
  {
    "question": "How does DFPO improve training stability and generalization compared to other methods?",
    "answer": "DFPO improves training stability and generalization by integrating conditional risk control and consistency constraints along value flow trajectories, which helps to stabilize training under noisy feedback.",
    "doc_id": "2602.05890v1",
    "gold_doc_ids": [
      "2602.05890v1"
    ],
    "title": "DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training",
    "question_id": "q_063"
  },
  {
    "question": "What is the name of the robust distributed GPU environment designed for reinforcement learning in kernel generation?",
    "answer": "The robust distributed GPU environment designed for reinforcement learning in kernel generation is called KernelGYM. It supports reward hacking checks, data collection from multi-turn interactions, and long-term RL training.",
    "doc_id": "2602.05885v1",
    "gold_doc_ids": [
      "2602.05885v1"
    ],
    "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations",
    "question_id": "q_064"
  },
  {
    "question": "What percentage of the generated kernels by Dr.Kernel-14B achieves at least a 1.2x speedup over the Torch reference on the KernelBench Level-2 subset?",
    "answer": "On the KernelBench Level-2 subset, 31.6% of the generated kernels by Dr.Kernel-14B achieve at least a 1.2x speedup over the Torch reference. This performance surpasses that of Claude-4.5-Sonnet and GPT-5.",
    "doc_id": "2602.05885v1",
    "gold_doc_ids": [
      "2602.05885v1"
    ],
    "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations",
    "question_id": "q_065"
  },
  {
    "question": "What languages does EuroLLM-22B support?",
    "answer": "EuroLLM-22B supports all 24 official European Union languages and 11 additional languages. This broad language coverage aims to address the underrepresentation of European languages in existing models.",
    "doc_id": "2602.05879v1",
    "gold_doc_ids": [
      "2602.05879v1"
    ],
    "title": "EuroLLM-22B: Technical Report",
    "question_id": "q_066"
  },
  {
    "question": "What key features of EuroLLM-22B are highlighted in the report?",
    "answer": "The report provides an overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. It also discusses the model's strong performance in reasoning, instruction following, and translation across multilingual benchmarks.",
    "doc_id": "2602.05879v1",
    "gold_doc_ids": [
      "2602.05879v1"
    ],
    "title": "EuroLLM-22B: Technical Report",
    "question_id": "q_067"
  },
  {
    "question": "What is the main purpose of the xList-Hate framework introduced in the paper?",
    "answer": "The xList-Hate framework aims to decompose hate speech detection into a checklist of explicit, concept-level questions, enabling robust and interpretable predictions without directly predicting the final label. It seeks to address the limitations of traditional supervised models that often overfit to dataset-specific definitions.",
    "doc_id": "2602.05874v1",
    "gold_doc_ids": [
      "2602.05874v1"
    ],
    "title": "xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection",
    "question_id": "q_068"
  },
  {
    "question": "How does the xList-Hate framework produce its diagnostic representation of hateful content?",
    "answer": "The framework uses a large language model to independently answer each checklist question, resulting in a binary diagnostic representation that captures features of hateful content. These diagnostic signals are then aggregated using a lightweight, fully interpretable decision tree.",
    "doc_id": "2602.05874v1",
    "gold_doc_ids": [
      "2602.05874v1"
    ],
    "title": "xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection",
    "question_id": "q_069"
  },
  {
    "question": "What is the main contribution of the research paper 'Constrained Group Relative Policy Optimization'?",
    "answer": "The main contribution is the introduction of Constrained GRPO, a Lagrangian-based extension of Group Relative Policy Optimization (GRPO) for constrained policy optimization. This approach enables direct optimization of violation rates through a Lagrangian relaxation.",
    "doc_id": "2602.05863v1",
    "gold_doc_ids": [
      "2602.05863v1"
    ],
    "title": "Constrained Group Relative Policy Optimization",
    "question_id": "q_070"
  },
  {
    "question": "What issue does the paper identify with naive multi-component treatment in advantage estimation?",
    "answer": "The paper identifies that a naive multi-component treatment can break constrained learning due to mismatched component-wise standard deviations, which distort the relative importance of different objective terms and corrupt the Lagrangian signal. This prevents meaningful constraint enforcement.",
    "doc_id": "2602.05863v1",
    "gold_doc_ids": [
      "2602.05863v1"
    ],
    "title": "Constrained Group Relative Policy Optimization",
    "question_id": "q_071"
  },
  {
    "question": "What is DLM-Scope, and what is its significance in the context of diffusion language models?",
    "answer": "DLM-Scope is the first sparse autoencoder (SAE)-based interpretability framework for diffusion language models (DLMs). Its significance lies in enabling the extraction of interpretable features and demonstrating that SAE insertion can reduce cross-entropy loss in DLMs, unlike in autoregressive large language models.",
    "doc_id": "2602.05859v1",
    "gold_doc_ids": [
      "2602.05859v1"
    ],
    "title": "DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders",
    "question_id": "q_072"
  },
  {
    "question": "How do sparse autoencoders (SAEs) affect diffusion language models (DLMs) compared to autoregressive large language models (LLMs)?",
    "answer": "Inserting SAEs in DLMs can reduce cross-entropy loss when applied to early layers, a contrast to LLMs where SAE insertion typically incurs a loss penalty. This difference indicates that SAEs affect DLMs more positively, enabling more effective diffusion-time interventions.",
    "doc_id": "2602.05859v1",
    "gold_doc_ids": [
      "2602.05859v1"
    ],
    "title": "DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders",
    "question_id": "q_073"
  },
  {
    "question": "What is the primary strategy employed by RRAttention to achieve dynamic sparse attention?",
    "answer": "RRAttention employs a head round-robin (RR) sampling strategy, which rotates query sampling positions across attention heads within each stride. This approach maintains query independence while enabling efficient global pattern discovery with stride-level aggregation.",
    "doc_id": "2602.05853v1",
    "gold_doc_ids": [
      "2602.05853v1"
    ],
    "title": "RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference",
    "question_id": "q_074"
  },
  {
    "question": "How does RRAttention improve computational efficiency compared to traditional attention mechanisms?",
    "answer": "RRAttention reduces the complexity from $O(L^2)$ to $O(L^2/S^2)$, allowing it to compute only half of the attention blocks while recovering over 99% of full attention performance. This results in a speedup of 2.4 times at a context length of 128K.",
    "doc_id": "2602.05853v1",
    "gold_doc_ids": [
      "2602.05853v1"
    ],
    "title": "RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference",
    "question_id": "q_075"
  },
  {
    "question": "What is the main purpose of introducing Surjective Pseudo-invertible Neural Networks (SPNN) in this paper?",
    "answer": "The main purpose of introducing SPNN is to provide a class of architectures explicitly designed to admit a tractable non-linear Pseudo-inverse (PInv) and to satisfy fundamental geometric properties, including null-space projection.",
    "doc_id": "2602.06042v1",
    "gold_doc_ids": [
      "2602.06042v1"
    ],
    "title": "Pseudo-Invertible Neural Networks",
    "question_id": "q_076"
  },
  {
    "question": "What method is formalized in the paper to guarantee consistency for non-linear mappings?",
    "answer": "The paper formalizes Non-Linear Back-Projection (NLBP) as a method that guarantees the same consistency constraint for non-linear mappings, ensuring that it satisfies the condition for the defined non-linear PInv.",
    "doc_id": "2602.06042v1",
    "gold_doc_ids": [
      "2602.06042v1"
    ],
    "title": "Pseudo-Invertible Neural Networks",
    "question_id": "q_077"
  },
  {
    "question": "What is the purpose of the CAMCUE framework introduced in the study?",
    "answer": "The CAMCUE framework is designed to use camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning, enabling a coherent 3D understanding from multi-view observations. It injects per-view pose into visual tokens and grounds natural-language viewpoint descriptions to a target camera pose.",
    "doc_id": "2602.06041v1",
    "gold_doc_ids": [
      "2602.06041v1"
    ],
    "title": "Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning",
    "question_id": "q_078"
  },
  {
    "question": "How many training and test instances are included in the CAMCUE-DATA dataset?",
    "answer": "The CAMCUE-DATA dataset consists of 27,668 training instances and 508 test instances that pair multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions.",
    "doc_id": "2602.06041v1",
    "gold_doc_ids": [
      "2602.06041v1"
    ],
    "title": "Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning",
    "question_id": "q_079"
  },
  {
    "question": "What are the three reasoning modes that SwimBird can switch among?",
    "answer": "SwimBird can dynamically switch among three reasoning modes: text-only reasoning, vision-only reasoning (using continuous hidden states as visual thoughts), and interleaved vision-text reasoning.",
    "doc_id": "2602.06040v1",
    "gold_doc_ids": [
      "2602.06040v1"
    ],
    "title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs",
    "question_id": "q_080"
  },
  {
    "question": "What is the purpose of the hybrid autoregressive formulation used in SwimBird?",
    "answer": "The hybrid autoregressive formulation unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, enabling SwimBird to effectively perform reasoning across different modalities.",
    "doc_id": "2602.06040v1",
    "gold_doc_ids": [
      "2602.06040v1"
    ],
    "title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs",
    "question_id": "q_081"
  },
  {
    "question": "What is the primary innovation of the GeoThinker framework compared to existing integration strategies for spatial reasoning?",
    "answer": "GeoThinker shifts the paradigm from passive fusion to active perception, enabling the model to selectively retrieve geometric evidence based on its internal reasoning demands, rather than mixing features indiscriminately.",
    "doc_id": "2602.06037v1",
    "gold_doc_ids": [
      "2602.06037v1"
    ],
    "title": "Thinking with Geometry: Active Geometry Integration for Spatial Reasoning",
    "question_id": "q_082"
  },
  {
    "question": "What is the peak score achieved by GeoThinker on the VSI-Bench, and what does this indicate about its performance?",
    "answer": "GeoThinker achieved a peak score of 72.6 on the VSI-Bench, indicating that it sets a new state-of-the-art in spatial intelligence and demonstrates robust generalization in various complex downstream scenarios.",
    "doc_id": "2602.06037v1",
    "gold_doc_ids": [
      "2602.06037v1"
    ],
    "title": "Thinking with Geometry: Active Geometry Integration for Spatial Reasoning",
    "question_id": "q_083"
  },
  {
    "question": "What is the main objective of the InterPrior framework introduced in the research?",
    "answer": "The main objective of the InterPrior framework is to enable humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. This is achieved through a scalable framework that learns a unified generative controller via large-scale imitation pretraining and reinforcement learning.",
    "doc_id": "2602.06035v1",
    "gold_doc_ids": [
      "2602.06035v1"
    ],
    "title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions",
    "question_id": "q_084"
  },
  {
    "question": "What methods are used in InterPrior to improve the generalization of the distilled policy?",
    "answer": "InterPrior uses data augmentation with physical perturbations and reinforcement learning finetuning to improve the competence of the distilled policy on unseen goals and initializations. These methods help consolidate the reconstructed latent skills into a valid manifold, allowing for generalization beyond the training data.",
    "doc_id": "2602.06035v1",
    "gold_doc_ids": [
      "2602.06035v1"
    ],
    "title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions",
    "question_id": "q_085"
  },
  {
    "question": "What is the main purpose of the V-Retrver framework proposed in the research?",
    "answer": "The main purpose of the V-Retrver framework is to reformulate multimodal retrieval as an agentic reasoning process grounded in visual inspection, allowing the Multimodal Large Language Model (MLLM) to selectively acquire visual evidence during reasoning.",
    "doc_id": "2602.06034v1",
    "gold_doc_ids": [
      "2602.06034v1"
    ],
    "title": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval",
    "question_id": "q_086"
  },
  {
    "question": "What strategies are combined in the training of the evidence-gathering retrieval agent?",
    "answer": "The training of the evidence-gathering retrieval agent combines a curriculum-based learning strategy that includes supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective.",
    "doc_id": "2602.06034v1",
    "gold_doc_ids": [
      "2602.06034v1"
    ],
    "title": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval",
    "question_id": "q_087"
  },
  {
    "question": "What framework is introduced in the paper to enhance 3D awareness in 2D Vision Foundation Models?",
    "answer": "The framework introduced is called Splat and Distill, which augments the teacher model with a fast, feed-forward 3D reconstruction pipeline to instill robust 3D awareness into 2D VFMs.",
    "doc_id": "2602.06032v1",
    "gold_doc_ids": [
      "2602.06032v1"
    ],
    "title": "Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation",
    "question_id": "q_088"
  },
  {
    "question": "What types of downstream tasks were evaluated in the study?",
    "answer": "The study conducted a comprehensive evaluation on tasks including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation.",
    "doc_id": "2602.06032v1",
    "gold_doc_ids": [
      "2602.06032v1"
    ],
    "title": "Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation",
    "question_id": "q_089"
  },
  {
    "question": "What is the main problem addressed by the proposed Context Forcing framework in video generation?",
    "answer": "The main problem addressed is the student-teacher mismatch, where the short-context teacher cannot guide the long-context student effectively due to its limited access to long-term history. This mismatch caps the student's context length and hinders its ability to understand global temporal dependencies.",
    "doc_id": "2602.06028v1",
    "gold_doc_ids": [
      "2602.06028v1"
    ],
    "title": "Context Forcing: Consistent Autoregressive Video Generation with Long Context",
    "question_id": "q_090"
  },
  {
    "question": "How does the Context Forcing framework enhance the training of long-context video generation models?",
    "answer": "Context Forcing enhances training by using a long-context teacher who is aware of the full generation history, thus eliminating the supervision mismatch. This allows for robust training of models capable of maintaining long-term consistency across longer video sequences.",
    "doc_id": "2602.06028v1",
    "gold_doc_ids": [
      "2602.06028v1"
    ],
    "title": "Context Forcing: Consistent Autoregressive Video Generation with Long Context",
    "question_id": "q_091"
  },
  {
    "question": "What are the two main advantages of the MambaVF framework compared to existing video fusion methods?",
    "answer": "MambaVF significantly reduces computation and memory costs by capturing long-range temporal dependencies with linear complexity and eliminates the need for explicit motion estimation. Additionally, it achieves high efficiency by reducing up to 92.25% of parameters and 88.79% of computational FLOPs.",
    "doc_id": "2602.06017v1",
    "gold_doc_ids": [
      "2602.06017v1"
    ],
    "title": "MambaVF: State Space Model for Efficient Video Fusion",
    "question_id": "q_092"
  },
  {
    "question": "What specific tasks does MambaVF achieve state-of-the-art performance in?",
    "answer": "MambaVF achieves state-of-the-art performance in multi-exposure, multi-focus, infrared-visible, and medical video fusion tasks. This demonstrates its effectiveness across various video processing applications.",
    "doc_id": "2602.06017v1",
    "gold_doc_ids": [
      "2602.06017v1"
    ],
    "title": "MambaVF: State Space Model for Efficient Video Fusion",
    "question_id": "q_093"
  },
  {
    "question": "What is the main goal of the VisRefiner framework in the context of screenshot-to-code generation?",
    "answer": "The main goal of the VisRefiner framework is to enable models to learn from visual differences between rendered predictions and reference designs, thereby improving the translation of user interface screenshots into executable frontend code. This is achieved through difference-aligned supervision that associates visual discrepancies with corresponding code edits.",
    "doc_id": "2602.05998v1",
    "gold_doc_ids": [
      "2602.05998v1"
    ],
    "title": "VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation",
    "question_id": "q_094"
  },
  {
    "question": "How does VisRefiner improve the code generation process compared to existing multimodal large language models?",
    "answer": "VisRefiner improves the code generation process by incorporating a reinforcement learning stage for self-refinement, where the model observes rendered outputs and target designs to identify visual differences and update the code accordingly. This iterative learning process enhances single-step generation quality and layout fidelity.",
    "doc_id": "2602.05998v1",
    "gold_doc_ids": [
      "2602.05998v1"
    ],
    "title": "VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation",
    "question_id": "q_095"
  },
  {
    "question": "What are the components of the proposed Multi-scale Global-Instance Prompt Tuning (MGIPT)?",
    "answer": "MGIPT consists of an Adaptive-scale Instance Prompt (AIP) and a Multi-scale Global-level Prompt (MGP). AIP dynamically learns lightweight and instance-specific prompts, while MGP captures domain-level knowledge across different scales.",
    "doc_id": "2602.05937v1",
    "gold_doc_ids": [
      "2602.05937v1"
    ],
    "title": "Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation",
    "question_id": "q_096"
  },
  {
    "question": "What limitations of existing Continual Test-Time Adaptation (CTTA) methods does the MGIPT aim to address?",
    "answer": "MGIPT aims to address limitations such as lacking multi-scale prompt diversity, inadequate incorporation of instance-specific knowledge, and the risk of privacy leakage. These issues arise from existing CTTA methods that rely on incrementally updating model parameters.",
    "doc_id": "2602.05937v1",
    "gold_doc_ids": [
      "2602.05937v1"
    ],
    "title": "Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation",
    "question_id": "q_097"
  },
  {
    "question": "What is the primary challenge that CLIP faces, according to the abstract?",
    "answer": "CLIP suffers from high memory and computation cost, which restricts its usage in resource-limited application scenarios. This high cost is a significant barrier to its widespread adoption.",
    "doc_id": "2602.05909v1",
    "gold_doc_ids": [
      "2602.05909v1"
    ],
    "title": "CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression",
    "question_id": "q_098"
  },
  {
    "question": "What novel framework does the paper propose for CLIP compression?",
    "answer": "The paper proposes a novel mapping-based CLIP compression framework called CLIP-Map. This framework utilizes learnable matrices to map and combine pretrained weights to preserve information from the original weights.",
    "doc_id": "2602.05909v1",
    "gold_doc_ids": [
      "2602.05909v1"
    ],
    "title": "CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression",
    "question_id": "q_099"
  }
]