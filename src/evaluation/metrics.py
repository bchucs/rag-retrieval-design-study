"""Evaluation metrics for RAG systems

Implements metrics for retrieval quality and answer correctness.
"""

from typing import List, Dict, Any, Optional
import logging
from openai import OpenAI

logger = logging.getLogger(__name__)


def retrieval_recall(
    retrieved_doc_ids: List[str],
    gold_doc_ids: List[str]
) -> float:
    """Calculate retrieval recall

    Args:
        retrieved_doc_ids: IDs of retrieved documents
        gold_doc_ids: IDs of gold/relevant documents

    Returns:
        Recall score (fraction of gold docs retrieved)
    """
    if not gold_doc_ids:
        return 0.0

    retrieved_set = set(retrieved_doc_ids)
    gold_set = set(gold_doc_ids)

    hits = len(retrieved_set.intersection(gold_set))
    return hits / len(gold_set)


def answer_correctness_score(
    generated_answer: str,
    reference_answer: str,
    question: str,
    client: Optional[OpenAI] = None,
    method: str = "llm_judge"
) -> float:
    """Evaluate answer correctness using LLM-as-judge

    Args:
        generated_answer: Generated answer from RAG system
        reference_answer: Reference/gold answer
        question: The original question
        client: OpenAI client (optional, will create if not provided)
        method: Evaluation method (currently only "llm_judge" supported)

    Returns:
        Correctness score on 0-2 scale:
        - 0: Incorrect or contradictory
        - 1: Partially correct or incomplete
        - 2: Fully correct
    """
    if method != "llm_judge":
        raise ValueError(f"Unsupported evaluation method: {method}")

    if client is None:
        client = OpenAI()

    prompt = f"""You are evaluating the correctness of an answer generated by a question-answering system.

Question: {question}

Reference Answer (Ground Truth): {reference_answer}

Generated Answer: {generated_answer}

Compare the generated answer to the reference answer and rate its correctness on a scale of 0-2:
- 0: Incorrect, contradictory, or missing key information
- 1: Partially correct but incomplete or contains minor errors
- 2: Fully correct and covers the key points from the reference answer

Consider:
- Factual accuracy
- Completeness of key information
- Technical correctness
- Whether claims align with the reference

Respond with ONLY a single number: 0, 1, or 2."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert evaluator for question-answering systems. Provide objective, consistent ratings."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.0,
            max_tokens=10,
        )

        content = response.choices[0].message.content.strip()

        # Parse score
        try:
            score = float(content)
            if score not in [0, 1, 2]:
                logger.warning(f"LLM returned invalid score: {content}, defaulting to 0")
                score = 0.0
        except ValueError:
            logger.warning(f"Could not parse LLM score: {content}, defaulting to 0")
            score = 0.0

        return score

    except Exception as e:
        logger.error(f"Error in answer correctness evaluation: {e}")
        return 0.0


def hallucination_detection(
    answer: str,
    context_documents: List[Dict[str, Any]],
    question: str,
    client: Optional[OpenAI] = None
) -> bool:
    """Detect if answer contains hallucinations using LLM-based verification

    Args:
        answer: Generated answer
        context_documents: Retrieved context documents
        question: The original question
        client: OpenAI client (optional, will create if not provided)

    Returns:
        True if hallucination detected, False otherwise (answer is grounded)
    """
    if client is None:
        client = OpenAI()

    # Build context from documents
    context_parts = []
    for i, doc in enumerate(context_documents):
        text = doc.get('text', '')
        title = doc.get('original_doc', {}).get('title', 'Unknown')
        context_parts.append(f"[Document {i+1}] {title}\n{text}")

    context = "\n\n".join(context_parts)

    prompt = f"""You are verifying whether an answer is grounded in the provided context documents.

Question: {question}

Context Documents:
{context}

Generated Answer: {answer}

Determine if the answer contains hallucinations (information not supported by the context):
- If all claims in the answer can be verified from the context documents, respond with "NO"
- If the answer contains information that cannot be found or is contradicted by the context, respond with "YES"

Consider:
- Does the answer cite facts not present in the context?
- Does the answer make specific claims that contradict the context?
- Is the answer's information actually derivable from the context?

Respond with ONLY one word: YES or NO."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert at detecting hallucinations in AI-generated text. Be strict and thorough."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.0,
            max_tokens=10,
        )

        content = response.choices[0].message.content.strip().upper()

        if "YES" in content:
            return True  # Hallucination detected
        elif "NO" in content:
            return False  # No hallucination
        else:
            logger.warning(f"Unexpected hallucination detection response: {content}, defaulting to False")
            return False

    except Exception as e:
        logger.error(f"Error in hallucination detection: {e}")
        return False  # Default to no hallucination on error